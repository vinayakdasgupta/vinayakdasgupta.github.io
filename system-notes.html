<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Vinayak Das Gupta – Writing</title>
  <link rel="stylesheet" href="/assets/css/style.css">
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@400;600&display=swap" rel="stylesheet">
</head>
<body>
  <header>
    <nav>
      <a href="index.html">Home</a>
      <a href="projects.html">Projects</a>
      <a href="writing.html">Writing</a>
      <a href="cv.html">CV</a>
      <a href="contact.html">Contact</a>
    </nav>
  </header>

  <main class="content">
    <h1>System notes</h1>

    <section class="blog-post">
        <h2>800 ≠ 200: The Panic and the Log Line</h2>
        <p>It began, as these things often do, with a log file. It was followed by a panic attack.</p>
        <p>Late one evening, I was going over training logs for <em>anvay</em> — my Bengali topic modelling tool — when I saw this line:<br>
        <code>merging changes from 200 documents into a model of 800 documents</code></p>
        <p>Something inside me stopped. I had claimed, quite confidently, in a paper submitted to a reputed peer-reviewed journal that <em>anvay</em> could process up to 800 documents. That it did so with a large vocabulary, over a million tokens, and in 62 seconds. Just to put it in context, the Gensim library (on which <em>anvay</em> is developed), out of the box, is about six times slower. Mallet (another popular topic modelling tool) is much, much slower. So, this was important to me. I had timed it. I had tested it. I had written about it. But this line suggested that something else was happening. That Gensim, the library handling the topic model, was only using 200 documents. And worse, it wasn't raising an error. It was merging them. I was caught off-guard.</p>
        <p>This wasn’t just a bug. If true, it meant that everything I had claimed, all the benchmarks, all the performance tests, all the carefully worded assertions about scalability and robustness was wrong.</p>
        <p>The panic that followed was entirely disproportionate and also completely familiar. A kind of tight, academic horror. Not because a tool might be broken, but because a mistake might have already been published. Because the act of writing had already fossilised something I might now have to recant.</p>
        <p>I went line by line through the code. I turned off all filters. I rechecked the tokenizer. I reran the same data under different configurations. Still: 200. I began rehearsing how I might write back to the editors. <br>
        <em>‘I regret to inform you that the benchmarks were inaccurate...’</em></p>
        <p>The turning point came, strangely enough, not from insight, but from exhaustion. I reduced the <code>chunksize</code> parameter — the number of documents Gensim processes in a single batch during training — from 200 to 64.</p>
        <p>The log now read:<br>
        <code>merging changes from 64 documents into a model of 800 documents</code></p>
        <p>That was it. That was all. The first number was the current chunk. The second was the total model. Gensim had never truncated anything. The system had always worked.</p>
        <p>The line was never proof of error. It was evidence of function. I simply didn't know how to read it.</p>
        <p>The <code>chunksize</code> parameter in Gensim’s LDA controls how many documents are processed in each batch during training. This is clear enough. What was unclear to me is this: when you see a log like “merging changes from 200 documents into a model of 800 documents,” it doesn’t mean only 200 documents were used. It means that Gensim has just processed one batch (of 200) and is now integrating that batch’s updates into the full 800-document model. The phrase reflects the incremental, online nature of training, where the global model is gradually updated from successive chunks. It’s a normal part of how <code>LdaMulticore</code> handles scalability and memory efficiency. I am still unsure if that line is the best way to communicate this.</p>
        <p>This would be a minor story if not for what it revealed about the emotional structure of academic work. The fear was not technical. It was social. It was the fear of being wrong in public. Of having claimed something and having it quietly unravel.</p>
        <p>There is a pressure in scholarly software to be unimpeachable. To be right, and to be right early. To anticipate every misunderstanding, every failure mode. But code isn’t like that. Neither is writing.</p>
        <p>What I had was a system that worked. What I lacked, for a moment, was the confidence to believe that I hadn’t faked it.</p>
        <p>What held up, in the end, was not just the pipeline. It was the process. <br>
        <em>anvay</em> wasn’t broken; I just needed to trust that I had done the work in the right way.</p>
      </section>
      
    
    <hr>
    
    <section class="blog-post">
        <h2>When the System Surprises You</h2>
        <h4>On emergence, authorship, and the quiet astonishment of generative art</h4>
      
        <p>The study of generative art arrived in my work through teaching. While designing a postgraduate course on art and technology, I found myself confronted with a curious category of works: visual artefacts generated not by hand, but by system. These were unmistakably aesthetic, sometimes astonishingly so.</p>
      
        <p>The question I began with was deceptively simple: how do we study art that was not composed in the traditional sense, but emerged?</p>
      
        <p>Generative art, in one of its more widely accepted definitions, is art in which an autonomous system contributes materially to the output. It is not random, nor is it fully authored. It sits somewhere in between. The artist constructs the constraints, sets the process in motion, but the final form is unknown until it appears. This unknowability — what systems theorists call emergence — is not a failure of control. It is the very condition of generativity.</p>
      
        <p>Let me make this concrete.</p>
      
        <p>I once wrote a program to generate an image I titled <em>Barcelona</em>. It is built on a system of Voronoi tessellations. The visual outcome, an abstract composition of coloured circles and black rectangles, is produced by distributing 6000 points across a canvas. These points are not randomly scattered, but weighted by the brightness values of a hidden reference image. Five colours are used. Each run of the program creates a new composition. No two are alike.</p>
      
        <p>To an external observer, <em>Barcelona</em> might seem composed, even calculated. But I know better. I know that while I chose the colours, the number of points, and the structure of the algorithm, I cannot determine the final image. I can control the rules, not the result.</p>
      
        <p>This is emergence. And it manifests in two distinct modes.</p>

        <div style="text-align: center; margin-bottom: 1.5rem;">
            <img src="/assets/img/barcelona.PNG" alt="Barcelona - generative artwork" style="max-width: 100%; height: auto; border: 1px solid #ddd; padding: 4px; background: #f9f9f9;">
        </div>
      
        <p>In one, <strong>objective emergence</strong>, we acknowledge the limits of our predictive power. The system behaves in ways that cannot be reduced to simpler descriptions. The only way to know what it will do is to run it. This is a familiar intuition to anyone who has worked in data analysis: simulation becomes epistemology.</p>
      
        <p>In the other, <strong>subjective emergence</strong>, the system surprises even when we understand it completely. It is not our ignorance that produces mystery, but the aesthetics of perception. We are startled by what we already know.</p>
      
        <p>Both modes challenge traditional ideas of authorship. They also challenge how we evaluate art. If each run of the algorithm generates a different output, which one is the work? If surprise is engineered, does it lose its value?</p>
      
        <p>Perhaps most provocatively, emergence invites us to reconsider the role of the artist. The value of a generative work lies not in its uniqueness, but in its capacity to generate uniqueness within constraint. The artist becomes less a composer and more a constructor of possibility.</p>
      
        <p>This, too, has implications for how we study such works. To appreciate a generative piece is not merely to look at it, but to understand the system that gave rise to it. In that sense, the aesthetics of generative art are inseparable from its mechanics. The code is not behind the art: it <em>is</em> the art.</p>
      
        <p>Some critics have argued that such works lack soul. That if emotion is absent at the point of generation, it cannot be recovered by the viewer. But this misunderstands the nature of system-based creation. Every line of code is a decision, every constraint a gesture. The emergence of form does not negate its meaning. It refracts it.</p>
      
        <p>The ink painting left to bleed. The salt print slowly darkening. The algorithm letting go. These are not separate categories. They are all moments where something more is produced than what was put in.</p>
      
        <p>And if we are moved by what we see, does it really matter how it was made?</p>
      </section>
      
    
    <hr>
    
    <section class="blog-post">
        <h2>Digital Humanities Didn’t Begin with the Computer</h2>
      
        <p>Every discipline has its origin myth. For Digital Humanities, it is often the story of Father Roberto Busa and the <em>Index Thomisticus</em>: a monumental mid-20th century effort to encode the works of Thomas Aquinas using IBM punch cards. It is an impressive origin story that really writes itself: how a clergyman met and tamed the machine. It is the story many of us learn in the first decade of the 21st century.</p>
      
        <p>But what if this isn’t where it began? What if tying the origins of a field to a machine — a particular instantiation of technology at a particular moment — obscures something older, more essential?</p>
      
        <p>My concern is not with the figure of Busa, but with what his positioning implies: that Digital Humanities is defined by its relationship to the modern computer. That computation and the computer are one and the same. This, I would argue, is both historically and conceptually misleading.</p>
      
        <p>Computation precedes the computer. Long before circuits and silicon, there were systems of reckoning. The abacus, Napier’s bones, Leibniz’s calculator, Babbage’s Analytical Engine — each a step in a lineage of mathematical imagination. The computer is not the origin of computation; it is one of its many manifestations. If we think of Digital Humanities as computationally inflected humanistic inquiry, then its history cannot begin with the mid-twentieth century. It must begin much earlier, in the moments when pattern, structure, and formal constraint became tools for thinking in the humanities.</p>
      
        <p>Take literature, for instance. The claim that literature resists mathematical thinking — that it is unrepeatable, affective, beyond system — is, I suspect, more myth than fact. Literature has long been shaped by form, rule, and design. The sonnet is a constraint. So is tragedy in five acts. So is the rhyme scheme of Dante’s <em>Divine Comedy</em>. Mathematical imagination is not an intrusion into literature; it is one of its substrates.</p>
      
        <p>Consider Vladimir Propp’s <em>Morphology of the Folktale</em> (1928). From a modest corpus of one hundred Russian fairy tales, Propp extracted a system of 31 functions, basic narrative moves that always occur in a fixed order. It is a kind of proto-algorithmic thinking. A recognition that stories are not infinite, but patterned. That imagination has structure.</p>
      
        <p>Or consider the Oulipo group, founded in the 1960s, a gathering of writers and mathematicians who used constraint as a source of creative energy. For them, the literary work was a combinatorial engine. Form came first. Meaning would follow, if it could.</p>
      
        <p>If this is computation — understood as rule-based generation and systemic variation — then surely it existed long before the computer. And if Digital Humanities is about these modes of inquiry, then it too predates its name.</p>
      
        <p><strong>So why does this matter?</strong></p>
      
        <p>Because origin stories shape expectations. When we tie a field to a specific technology, we make it vulnerable to that technology’s limits, its obsolescence, its market logics. We confuse the machine with the method. We risk reducing thought to tool-use.</p>
      
        <p>We also risk absurdity. If the presence of a computer defines Digital Humanities, then every action on a device — from writing a document to checking email — becomes a DH act. This is clearly untenable. Not everything done on a computer is Digital Humanities. Nor must Digital Humanities always involve a computer.</p>
      
        <p>What matters is the disposition. A willingness to think structurally, to formalise ambiguity, to trace patterns in what seems singular. To ask: what else might this text or this image or this object reveal if we look at it sideways?</p>
      
        <p>And here, the computer is simply one more lens. Powerful, yes. But not foundational.</p>
      
        <p>I have no desire to unseat Busa. His work was remarkable, and symbolic in all the right ways. But perhaps it is time to write a different prelude to the field — one that begins not with machines, but with a form of thought. Not with a moment, but with a method.</p>
      
        <p><strong>Digital Humanities, then, is not defined by the digital. It is animated by the mathematical. And mathematics, quietly, but intently, has always been part of the humanities.</strong></p>
      </section>
      
  </main>

  <footer>
    <p>&copy; 2024 Vinayak Das Gupta. All rights reserved.</p>
  </footer>
</body>
</html>